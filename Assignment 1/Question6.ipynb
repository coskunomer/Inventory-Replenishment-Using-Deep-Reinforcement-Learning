{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9ohEKC7OJJE"
   },
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A2Ls5hp6Svkr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEesE0fNWNT5"
   },
   "source": [
    "In this question, we will try to come up with a faster and better solution for the logistics problem we have. While training both policy iteration and tabular q-learning algorithm in question 5, we struggled with the problem of very long training times and curses of dimensionality because of the big state and action space we had. To overcome this issue, we will make use of the observations we made from  our nearly optimal q-learning model we trained in question 5. First and most obvious observation that we can make for this question is that the route (1,2,3) is very costly and it should never be used since its cost is much higher than just bearing loss sales. Therefore, we will exclude this route from the problem in this part. By doing this, we will reduce action space from having 1331 elements (all combination of 3 integers between 0-10) to 331 elements (by subtracting  all combinations of 3 integers between 1-10).\n",
    "<br>\n",
    "\n",
    "We will also carry out a sensitive analysis to see how different cost values affects the decisions of our q-learning algorithm makes. To achieve this, we will train 3 different versions of this algorithm:\n",
    "\n",
    "- Original Version (excluding the route (1,2,3))\n",
    "- Original Version with holding cost being 10 instead of 1\n",
    "- Original version with higher route costs for routes (1,2), (1,3), (2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDdAp9gUWLET"
   },
   "source": [
    "# Original Version (excluding the route (1,2,3))\n",
    "\n",
    "Below code is the same as the one we used to train the q-learning in question 5. Therefore, you can refer there for the explanation of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "TgDEQ7ceRnB8",
    "outputId": "92cc736c-a4c5-4580-973a-e280642bbb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pre-trained q table\n",
      "mean order up to level: [6.95 8.16 7.51]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0)</th>\n",
       "      <th>(1,3)</th>\n",
       "      <th>(1, 2)</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(2)</th>\n",
       "      <th>(3)</th>\n",
       "      <th>(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>140</td>\n",
       "      <td>101</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (0)  (1,3)  (1, 2)  (2, 3)  (2)  (3)  (1)\n",
       "0   10     11     140     101   27    4    7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state space\n",
    "states = np.array([np.array([i,j,z]) for i in range(11) for j in range(11) for z in range(11)]) # inventory before the start of the day\n",
    "state_index = {}\n",
    "for i, s in enumerate(states):\n",
    "    state_index[tuple(s)] = i\n",
    "\n",
    "def inv_r(inv_level):\n",
    "    inv_reward1 = (max(0, inv_level[0]) * -1) + (min(0, inv_level[0]) * 19)\n",
    "    inv_reward2 = (max(0, inv_level[1]) * -1) + (min(0, inv_level[1]) * 19)\n",
    "    inv_reward3 = (max(0, inv_level[2]) * -1) + (min(0, inv_level[2]) * 19)\n",
    "    direct_reward = inv_reward1 + inv_reward2 + inv_reward3\n",
    "    return direct_reward\n",
    "\n",
    "def route_r(action):\n",
    "    if action[0] > 0 and action[1] > 0 and action[2] > 0:\n",
    "        route_reward = -500\n",
    "    elif action[0] > 0 and action[1] > 0:\n",
    "        route_reward = -60\n",
    "    elif action[0] > 0 and action[2] > 0:\n",
    "        route_reward = -75\n",
    "    elif action[1] > 0 and action[2] > 0:\n",
    "        route_reward = -75\n",
    "    elif action[0] > 0:\n",
    "        route_reward = -40\n",
    "    elif action[1] > 0:\n",
    "        route_reward = -40\n",
    "    elif action[2] > 0:\n",
    "        route_reward = -55\n",
    "    else:\n",
    "        route_reward = 0\n",
    "    return route_reward\n",
    "\n",
    "def get_route(action):\n",
    "    if action[0] > 0 and action[1] > 0 and action[2] > 0:\n",
    "        return \"(1, 2, 3)\"\n",
    "    elif action[0] > 0 and action[1] > 0:\n",
    "        return \"(1, 2)\"\n",
    "    elif action[0] > 0 and action[2] > 0:\n",
    "        return \"(1,3)\"\n",
    "    elif action[1] > 0 and action[2] > 0:\n",
    "        return \"(2, 3)\"\n",
    "    elif action[0] > 0:\n",
    "        return \"(1)\"\n",
    "    elif action[1] > 0:\n",
    "        return \"(2)\"\n",
    "    elif action[2] > 0:\n",
    "        return \"(3)\"\n",
    "    else:\n",
    "        return \"(0)\"\n",
    "\n",
    "def generate_demand():\n",
    "    d1 = min(10, np.ceil(np.random.gamma(shape=9, scale=1/3)))\n",
    "    d2 = min(10, np.ceil(np.random.gamma(shape=12.5, scale=0.4)))\n",
    "    d3 = min(10, np.ceil(np.random.gamma(shape=4/3, scale=1.5)))\n",
    "    return np.array([d1, d2, d3])\n",
    "\n",
    "get_actions = {}\n",
    "for s in states:\n",
    "    max_purchase = 10 - s\n",
    "    actions = np.array([np.array([i,j,z]) for i in range(max_purchase[0]+1) for j in range(max_purchase[1]+1) for z in range(max_purchase[2]+1) if not np.array([i,j,z]).all() > 0])\n",
    "    get_actions[tuple(s)] = actions\n",
    "\n",
    "# initializing the Q(s, a) table\n",
    "try:\n",
    "    with open(\"q_dict_q6.pkl\", \"rb\") as fl:\n",
    "        q_table = pickle.load(fl)\n",
    "        print(\"using pre-trained q table\")\n",
    "except:\n",
    "    print(\"using a new q table\")\n",
    "    np.random.seed(seed=1)\n",
    "    q_table = {}\n",
    "    for s in states:\n",
    "        actions = get_actions[tuple(s)]\n",
    "        q_table[tuple(s)] = {}\n",
    "        for a in actions:\n",
    "            q_table[tuple(s)][tuple(a)] = np.random.rand()\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "route_frequency = {}\n",
    "for n in range(1):\n",
    "    # randomly initializing the state\n",
    "    s = np.random.randint(low=0, high=11, size=3)\n",
    "    order_up_to = []\n",
    "\n",
    "    # if you are not going to use the pre-trained q-table,\n",
    "    # we advise you to change the # of iterations to at least 1 million.\n",
    "    for _ in range(300):\n",
    "        tuple_s = tuple(s)\n",
    "        q_dict = q_table[tuple_s]\n",
    "        index = state_index[tuple_s]\n",
    "        if np.random.rand() < epsilon:\n",
    "            A = get_actions[tuple_s]\n",
    "            action = A[np.random.randint(low=0, high=len(A))]\n",
    "        else:\n",
    "            action = max(q_dict, key=q_dict.get)\n",
    "        route = get_route(action)\n",
    "        route_frequency[route] = route_frequency.get(route, 0) + 1\n",
    "        d = generate_demand()\n",
    "        new_s = s + action  - d\n",
    "        reward = route_r(action) + inv_r(new_s)\n",
    "        new_s[new_s<0] = 0\n",
    "        new_q = max(q_table[tuple(new_s)].values())\n",
    "        tuple_a = tuple(action)\n",
    "        old_q = q_dict[tuple_a]\n",
    "        q_table[tuple_s][tuple_a] += alpha * (reward + 0.8 * new_q - old_q)\n",
    "        order_up_to.append(s+action)\n",
    "        #print(f\"state: {s}, action: {action}, inv: {s+action-d}, demand: {d}, reward: {reward}\")\n",
    "        s = np.asarray(new_s.copy()).astype(int)\n",
    "print(f\"mean order up to level: {np.round(np.array(order_up_to).mean(axis=0), 2)}\")\n",
    "pd.DataFrame([route_frequency])\n",
    "#with open(\"q_dict_q6.pkl\", \"wb\") as fl:\n",
    "#    pickle.dump(q_table, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "966CwGvQuZpe"
   },
   "source": [
    "# Original Version with holding cost being 10 instead of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "7NONcmVjubby",
    "outputId": "3f4e34de-4a8f-444b-9cd0-d4d39aa1634e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pre-trained q table\n",
      "mean order up to level: [4.54 6.22 2.74]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0)</th>\n",
       "      <th>(2)</th>\n",
       "      <th>(1, 2)</th>\n",
       "      <th>(1,3)</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(1)</th>\n",
       "      <th>(3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>154</td>\n",
       "      <td>19</td>\n",
       "      <td>91</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (0)  (2)  (1, 2)  (1,3)  (2, 3)  (1)  (3)\n",
       "0    3   18     154     19      91   10    5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, we changed inventory holding cost to -10 from -1\n",
    "def inv_r(inv_level):\n",
    "    inv_reward1 = (max(0, inv_level[0]) * -10) + (min(0, inv_level[0]) * 19)\n",
    "    inv_reward2 = (max(0, inv_level[1]) * -10) + (min(0, inv_level[1]) * 19)\n",
    "    inv_reward3 = (max(0, inv_level[2]) * -10) + (min(0, inv_level[2]) * 19)\n",
    "    direct_reward = inv_reward1 + inv_reward2 + inv_reward3\n",
    "    return direct_reward\n",
    "\n",
    "# initializing the Q(s, a) table\n",
    "try:\n",
    "    with open(\"q_dict_cost1.pkl\", \"rb\") as fl:\n",
    "        q_table = pickle.load(fl)\n",
    "        print(\"using pre-trained q table\")\n",
    "except:\n",
    "    print(\"using a new q table\")\n",
    "    np.random.seed(seed=1)\n",
    "    q_table = {}\n",
    "    for s in states:\n",
    "        actions = get_actions[tuple(s)]\n",
    "        q_table[tuple(s)] = {}\n",
    "        for a in actions:\n",
    "            q_table[tuple(s)][tuple(a)] = np.random.rand()\n",
    "\n",
    "epsilon = 0.\n",
    "alpha = 0.5\n",
    "route_frequency ={}\n",
    "for n in range(1):\n",
    "    # randomly initializing the state\n",
    "    s = np.random.randint(low=0, high=11, size=3)\n",
    "    order_up_to = []\n",
    "\n",
    "    # if you are not going to use the pre-trained q-table,\n",
    "    # we advise you to change the # of iterations to at least 1 million.\n",
    "    for _ in range(300):\n",
    "        tuple_s = tuple(s)\n",
    "        q_dict = q_table[tuple_s]\n",
    "        index = state_index[tuple_s]\n",
    "        if np.random.rand() < epsilon:\n",
    "            A = get_actions[tuple_s]\n",
    "            action = A[np.random.randint(low=0, high=len(A))]\n",
    "        else:\n",
    "            action = max(q_dict, key=q_dict.get)\n",
    "        route = get_route(action)\n",
    "        route_frequency[route] = route_frequency.get(route, 0) + 1\n",
    "        d = generate_demand()\n",
    "        new_s = s + action  - d\n",
    "        reward = route_r(action) + inv_r(new_s)\n",
    "        new_s[new_s<0] = 0\n",
    "        new_q = max(q_table[tuple(new_s)].values())\n",
    "        tuple_a = tuple(action)\n",
    "        old_q = q_dict[tuple_a]\n",
    "        q_table[tuple_s][tuple_a] += alpha * (reward + 0.8 * new_q - old_q)\n",
    "\n",
    "        order_up_to.append(s+action)\n",
    "        s = np.asarray(new_s.copy()).astype(int)\n",
    "print(f\"mean order up to level: {np.round(np.array(order_up_to).mean(axis=0), 2)}\")\n",
    "pd.DataFrame([route_frequency])\n",
    "#with open(\"q_dict_cost1.pkl\", \"wb\") as fl:\n",
    "#    pickle.dump(q_table, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwBOJj_FNeZC"
   },
   "source": [
    "# Original version with higher route costs for routes (1,2), (1,3), (2,3)\n",
    "\n",
    "We changed costs for the following routes to the values next to them.\n",
    "- Route (1, 2): 150 <br>\n",
    "- Route (1, 3): 180 <br>\n",
    "- Route (2, 3): 180 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "3oQhc8ZLNeMg",
    "outputId": "d1d5bbbe-4ad2-489b-83e5-d5301b5236e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pre-trained q table\n",
      "mean order up to level: [5.41 6.11 5.39]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(2)</th>\n",
       "      <th>(1, 2)</th>\n",
       "      <th>(1,3)</th>\n",
       "      <th>(3)</th>\n",
       "      <th>(1)</th>\n",
       "      <th>(0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (2, 3)  (2)  (1, 2)  (1,3)  (3)  (1)  (0)\n",
       "0      54   71      77     38   26   31    3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inv_r(inv_level):\n",
    "    inv_reward1 = (max(0, inv_level[0]) * -1) + (min(0, inv_level[0]) * 19)\n",
    "    inv_reward2 = (max(0, inv_level[1]) * -1) + (min(0, inv_level[1]) * 19)\n",
    "    inv_reward3 = (max(0, inv_level[2]) * -1) + (min(0, inv_level[2]) * 19)\n",
    "    direct_reward = inv_reward1 + inv_reward2 + inv_reward3\n",
    "    return direct_reward\n",
    "\n",
    "def route_r(action):\n",
    "    if action[0] > 0 and action[1] > 0 and action[2] > 0:\n",
    "        route_reward = -500\n",
    "    elif action[0] > 0 and action[1] > 0:\n",
    "        route_reward = -150\n",
    "    elif action[0] > 0 and action[2] > 0:\n",
    "        route_reward = -180\n",
    "    elif action[1] > 0 and action[2] > 0:\n",
    "        route_reward = -180\n",
    "    elif action[0] > 0:\n",
    "        route_reward = -40\n",
    "    elif action[1] > 0:\n",
    "        route_reward = -40\n",
    "    elif action[2] > 0:\n",
    "        route_reward = -55\n",
    "    else:\n",
    "        route_reward = 0\n",
    "    return route_reward\n",
    "\n",
    "# initializing the Q(s, a) table\n",
    "try:\n",
    "    with open(\"q_dict_cost2.pkl\", \"rb\") as fl:\n",
    "        q_table = pickle.load(fl)\n",
    "        print(\"using pre-trained q table\")\n",
    "except:\n",
    "    print(\"using a new q table\")\n",
    "    np.random.seed(seed=1)\n",
    "    q_table = {}\n",
    "    for s in states:\n",
    "        actions = get_actions[tuple(s)]\n",
    "        q_table[tuple(s)] = {}\n",
    "        for a in actions:\n",
    "            q_table[tuple(s)][tuple(a)] = np.random.rand()\n",
    "\n",
    "epsilon = 0.5\n",
    "alpha = 0.5\n",
    "route_frequency = {}\n",
    "for n in range(1):\n",
    "    # randomly initializing the state\n",
    "    s = np.random.randint(low=0, high=11, size=3)\n",
    "    order_up_to = []\n",
    "\n",
    "    # if you are not going to use the pre-trained q-table,\n",
    "    # we advise you to change the # of iterations to at least 1 million.\n",
    "    for _ in range(300):\n",
    "        tuple_s = tuple(s)\n",
    "        q_dict = q_table[tuple_s]\n",
    "        index = state_index[tuple_s]\n",
    "        if np.random.rand() < epsilon:\n",
    "            A = get_actions[tuple_s]\n",
    "            action = A[np.random.randint(low=0, high=len(A))]\n",
    "        else:\n",
    "            action = max(q_dict, key=q_dict.get)\n",
    "        route = get_route(action)\n",
    "        route_frequency[route] = route_frequency.get(route, 0) + 1\n",
    "        d = generate_demand()\n",
    "        new_s = s + action  - d\n",
    "        reward = route_r(action) + inv_r(new_s)\n",
    "        new_s[new_s<0] = 0\n",
    "        new_q = max(q_table[tuple(new_s)].values())\n",
    "        tuple_a = tuple(action)\n",
    "        old_q = q_dict[tuple_a]\n",
    "        q_table[tuple_s][tuple_a] += alpha * (reward + 0.8 * new_q - old_q)\n",
    "\n",
    "        order_up_to.append(s+action)\n",
    "        s = np.asarray(new_s.copy()).astype(int)\n",
    "print(f\"mean order up to level: {np.round(np.array(order_up_to).mean(axis=0), 2)}\")\n",
    "pd.DataFrame([route_frequency])\n",
    "#with open(\"q_dict_cost2.pkl\", \"wb\") as fl:\n",
    "#    pickle.dump(q_table, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqJhJYAyUbMa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
